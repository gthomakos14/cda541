---
title: "R Notebook"
output: html_notebook
---
1) (10 points) Consider the cereal dataset in UBlearns.  Suppose that you are getting 
this data in order to build a predictive model for nutritional rating. 

#############################################################################################
I'm loading in the pre-processed data from the previous homework assignment. The pre-processing that took place removed outliers, removed some useless columns (namely mfr, type, shelf, and weight), scaled the other columns to accomodate for serving size denoted in the cups column, and removed entries with missing values (in the original csv, indicated by a -1. The reason for total removal was due to the small quantity: there were only 3.)
#############################################################################################

```{r warning=FALSE}
rm(list=ls())
library(dplyr)
library(leaps)
library(class)
library(ISLR)
library(glmnet)
```

a) Divide the data into test (20% of data) and training (80% of data).  Fit a linear 
model and report the MSE.

#############################################################################################
The variable model_no_selection in the cell below is a fitted linear model on all variables within the adjusted cereal dataset. Both the test and the train MSE are printed.
#############################################################################################

```{r}
load(file="df_cereal.rdata")

set.seed(12)
df_train = sample_frac(df_cereal, 0.8)
df_test = anti_join(df_cereal, df_train, by='name')

model_no_selection = lm(rating~calories+protein+fat+sodium+fiber+carbo+sugars+potass+vitamins, data=df_train)

df_test$predicted = predict.lm(model_no_selection, df_test)
df_test$squared_error = (df_test$rating - df_test$predicted)^2
df_train$predicted = predict.lm(model_no_selection, df_train)
df_train$squared_error = (df_train$rating - df_train$predicted)^2

print(paste('The Test MSE is: ', mean(df_test$squared_error)))
print(paste('The Train MSE is: ', mean(df_train$squared_error)))

df_test = select(df_test, -one_of(c('predicted','squared_error')))
df_train = select(df_train, -one_of(c('predicted','squared_error')))
```

b) With the data in (a) perform forwards subset selection. 
c) With the data in (a) perform exhaustive subset selection. 

#############################################################################################
The cell below creates two models from the training data from the df_cereal variable. One, regit_fwd, was made using forward subset selection while regit_exh was made using exhausting subset selection. It also creates the summaries for each and plots a variety of stats for evaluation of each model's success at each quantity of variable. 

The cell following is a slight modification from the lab work, made to work for the cereal dataset rather than the Boston dataset, but the train and test errors are still printed out. For brevity's sake, I've only chosen to print out for the exhaustive subset selection, but this was also done with forward subset selection.

For both forward subset selection as well as exhaustive, the success of the model tops out at around 7 variables. Normally, I'd have chosen 7 or 8 features as the way to do things but both forward and exhaustive subset selection arrive at the same feature space so in an effort to get models that aren't identical, we're going to be using 6 features.
#############################################################################################

```{r}
# The reason for dropping the one column from the data is that it's the name of the cereal
regit_fwd = regsubsets(rating~., data=df_train[,-1], nbest=1, nvmax=10, method='forward')
regit_exh = regsubsets(rating~., data=df_train[,-1], nbest=1, nvmax=10, method='exhaustive')

fwd_summary = summary(regit_fwd)
exh_summary = summary(regit_exh)

par(mfrow = c(2,2))
plot(fwd_summary$cp, xlab = "No. of variables", ylab = "Cp", type = "l")
plot(fwd_summary$bic, xlab = "No. of variables", ylab = "BIC", type = "l")
plot(fwd_summary$rss, xlab = "No. of variables", ylab = "RSS", type = "l")
plot(fwd_summary$adjr2, xlab = "No. of variables", ylab = "Adjusted Rsq", type = "l")

par(mfrow = c(2,2))
plot(exh_summary$cp, xlab = "No. of variables", ylab = "Cp", type = "l")
plot(exh_summary$bic, xlab = "No. of variables", ylab = "BIC", type = "l")
plot(exh_summary$rss, xlab = "No. of variables", ylab = "RSS", type = "l")
plot(exh_summary$adjr2, xlab = "No. of variables", ylab = "Adjusted Rsq", type = "l")
```

d) Draw some conclusions through comparisons between models (a-c). Reflect on 
the comparative predictive accuracy, and model interpretation. Which model would 
you say is the “best one” based on your results?  Why? 

#############################################################################################
All of the models are quite interpretable given that they're all linear models. While the number of features between the full linear model and the ones that had subset selection differ, The distinction comes from the predictive accuracy of each model and for that we should be looking at the train MSE for each model which will be printed out in the code cell below.

From this, we can see the hierarchy of MSE's being "no selection is better than exhaustive is better than forward". However, it should be known that the subset selection models also use less features than no selection and are thus less computationally expensive. Furthermore, pre-processing of the data eliminated some variables which may have interfered with the accuracy of a model that uses the full data frame as the feature space.
#############################################################################################

```{r}
predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    mat[, names(coefi)] %*% coefi
}

df_test$exh_predict = predict(regit_exh, df_test, id=6)
df_test$fwd_predict = predict(regit_fwd, df_test, id=6)
df_test$no_select_predict = predict.lm(model_no_selection, df_test)

exh_mse = mean((df_test$exh_predict - df_test$rating)^2)
fwd_mse = mean((df_test$fwd_predict - df_test$rating)^2)
no_select_mse = mean((df_test$no_select_predict - df_test$rating)^2)

print(paste('The exhaustive selection MSE is: ',exh_mse))
print(paste('The forward selection MSE is: ',fwd_mse))
print(paste('The no selection MSE is: ',no_select_mse))
```

2) (10  points)  ESL  textbook  exercise  2.8  modified:    Compare  the  classification 
performance  of  linear  regression  and  k-nearest  neighbor  classification  on  the 
zipcode data.  In particular, consider only the 4’s and 7’s for this problem, and k = 
1,3,5,7,9,11,13,15.  Show both the training and the test error for each choice of k.  
The zipcode data is available in the ElemStatLearn package – or the website for the 
text ESL for download.  Note that you do not have to divide the data into test and 
training because it is done for you.  

#############################################################################################
I'm looking mostly at the test error rate for these two models since that's normally what you'd be using in order to judge how good your classifier is. From this we can see that the linear regression doesn't perform as well as any of the values of k. However, it should be noted that the training error is better than all values of k aside from k=1 but its error rate of 0 is a trivial response.
#############################################################################################

```{r}
# I couldn't handle the dot being there in the variable. Changed the variable
# name to fit with other variables in the notebook.
# Also changed it to data frame format instead of matrix format since I'm more
# familiar with that one.
load('zip_data/zip.test.RData')
load('zip_data/zip.train.RData')
zip_test = data.frame(zip.test)
zip_train = data.frame(zip.train)

# Filters out everything that isn't a 4 or a 7 then changes the key column to be 
# 1 or 0, 1 being a 4 and 7 being a 0 just to make future stuff simpler.
df_train = filter(zip_train, X1 %in% c(4,7))
df_train$X1[df_train$X1 == 4] = 1
df_train$X1[df_train$X1 == 7] = 0
df_test = filter(zip_test, X1 %in% c(4,7))
df_test$X1[df_test$X1 == 4] = 1
df_test$X1[df_test$X1 == 7] = 0
```

```{r warning=FALSE}
# Some code repetition here and this would likely be better as a function for
# aesthetic purposes but this works well and it'd be a VERY specialized function
# so I'm just leaving it as is.
zip_lm = lm(X1~.,data=df_train)
lm_predicted = predict(zip_lm, df_test)
df_test$predicted = lm_predicted
df_lm_predicted = select(df_test, c('X1','predicted')) %>%
  mutate('rounded_predicted' = round(predicted),
         'misclassified' = abs(X1-rounded_predicted))
print(paste('The test misclassification rate is: ',mean(df_lm_predicted$misclassified)))

lm_predicted = predict(zip_lm, df_train)
df_train$predicted = lm_predicted
df_lm_predicted = select(df_train, c('X1','predicted')) %>%
  mutate('rounded_predicted' = round(predicted),
         'misclassified' = abs(X1-rounded_predicted))
print(paste('The train misclassification rate is: ',mean(df_lm_predicted$misclassified)))
```

```{r}
knn_error_rate = function(df_train, df_test){
  possible_k_values = c(1,3,5,7,9,11,13,15)
  df_k_misclassification_error = data.frame(matrix(nrow=0,ncol=3))
  colnames(df_k_misclassification_error) = c('k','error_rate_test','error_rate_train')
  for(i in possible_k_values){
    i_nn_test = knn(df_train[,-1], df_test[,-1], df_train$X1, k=i)
    df_test_predictions = df_test
    df_test_predictions$predicted = as.numeric(i_nn_test)-1
    df_test_predictions = select(df_test_predictions, c('X1','predicted')) %>%
                            mutate('misclassified'=abs(X1-predicted))
    error_rate_test = mean(df_test_predictions$misclassified)
    
    i_nn_train = knn(df_train[,-1], df_train[,-1], df_train$X1, k=i)
    df_train_predictions = df_train
    df_train_predictions$predicted = as.numeric(i_nn_train)-1
    df_train_predictions = select(df_train_predictions, c('X1','predicted')) %>%
                            mutate('misclassified'=abs(X1-predicted))
    error_rate_train = mean(df_train_predictions$misclassified)
    df_k_misclassification_error[nrow(df_k_misclassification_error)+1,]=c(i, error_rate_test, error_rate_train)
  }
  return(df_k_misclassification_error)
}

knn_error_rate(df_train, df_test)
```

3) (10 points) In this exercise, we will predict the number of applications received 
using the  other variables in the College data set in the ISLR package. 

#############################################################################################
First, some transformations need to be done on the dataset. Namely, we need to adjust the scale of some of these columns. Since it uses total number of students for columns like Apps or Accept, this is just an indicator of the size of the school when we could be doing acceptance rate, enrollment rate, etc and get a better picture of the school.
#############################################################################################
```{r}
data(College)
df_college = College
# Cazenovia College has a graduation rate of 118%. We'll just assume that's a typo and bring that down to 100.
df_college['Cazenovia College', 'Grad.Rate']=100
# Rutgers at New Brunswick is also the school that got the largest number of applications by far, nearly double second 
# place so that will simply be removed.
df_college = filter(df_college, Apps<30000) %>%
  mutate('Accept.Rate' = Accept/Apps,
         'Enroll.Rate' = Enroll/Accept,
         'F.Undergrad.Pct' = F.Undergrad/Enroll,
         'P.Undergrad.Pct' = P.Undergrad/Enroll)
```

(a) Split the data set into a training set and a test set.  Fit a linear model using 
least squares on the training set and report the test error obtained. 

#############################################################################################
As can be seen printed below, this is an egregiously high MSE. However, this is to be expected since there's quite a few insignificant variables around muddying up the model and leading to a large amount of overfitting.
#############################################################################################
```{r}
set.seed(12)
# In keeping with the train-test split ratio of above, the data is split 80:20.
sample_size = floor(0.8*nrow(df_college))
index = sample(seq_len(nrow(df_college)),size=sample_size)
df_college_train = df_college[index,]
df_college_test = df_college[-index,]
```

```{r}
library(glmnet)
# The linear model uses all variables the dataset comes with, simply meant to exaggerate the feature selection 
# capabilities of the ridge regression and lasso techniques to follow immediately after.
linear_college = lm(Grad.Rate~., data=df_college_train)
df_college_test$Grad.Rate.Predicted = predict(linear_college, newdata = df_college_test)
df_college_test = mutate(df_college_test,
                         'Error'=Grad.Rate.Predicted-Grad.Rate,
                         'Error_Squared'=Error*Error)
print(paste('The test MSE with the OLS model is: ', mean(df_college_test$Error_Squared)))
```

(b) Fit a ridge regression model on the training set, with λ chosen by cross-
validation. Report the test error obtained.

#############################################################################################
As we can see, there is a small improvement in the test error compared to the OLS model. Again, this could definitely be improved by the outright removal of some features but that is not the purpose of this write-up.
#############################################################################################

```{r}
####### This is only here to refresh the datasets and allow me to use the same variable names ######
set.seed(12)
# In keeping with the train-test split ratio of above, the data is split 80:20.
sample_size = floor(0.8*nrow(df_college))
index = sample(seq_len(nrow(df_college)),size=sample_size)
df_college_train = df_college[index,]
df_college_test = df_college[-index,]
```


```{r}
set.seed(12)
x_train = data.matrix(select(df_college_train,-Grad.Rate))
y_train = df_college_train$Grad.Rate
x_test = data.matrix(select(df_college_test, -Grad.Rate))
lambda_finder = cv.glmnet(x_train, y_train, alpha=0)
best_lambda = lambda_finder$lambda.min

ridge_model = glmnet(x_train, y_train, alpha=0, lambda=best_lambda)
```

```{r}
df_college_test$Grad.Rate.Predicted = predict(ridge_model, s=best_lambda, newx=x_test)
df_college_test = mutate(df_college_test,
                         'Error'=Grad.Rate.Predicted-Grad.Rate,
                         'Error_Squared'=Error*Error)
print(paste('The test MSE with the ridge regression model is: ', mean(df_college_test$Error_Squared)))
```
(c) Fit a lasso model on the training set, with λ chosen by crossvalidation. 
Report the test error obtained, along with the number of non-zero coefficient 
estimates. 

#############################################################################################
This will be the same code as above, but just changing the alpha in the glmnet calls to 1.

While the error rate isn't much improved compared to the OLS model, what we're mostly after is the feature selection. However, none of the variables went to zero but if we were to override the cross-validation and choose a lambda larger than what was algorithmically chosen then some of these would inevitably go to zero. Some are even quite close to it already. Interestingly, the new columns created that were adjusted to be a "rate of" column rather than just a raw number (e.g. acceptance rate instead of total students accepted) performed much better than their counterparts.
#############################################################################################
```{r}
####### This is only here to refresh the datasets and allow me to use the same variable names ######
set.seed(12)
# In keeping with the train-test split ratio of above, the data is split 80:20.
sample_size = floor(0.8*nrow(df_college))
index = sample(seq_len(nrow(df_college)),size=sample_size)
df_college_train = df_college[index,]
df_college_test = df_college[-index,]
```

```{r}
set.seed(12)
x_train = data.matrix(select(df_college_train,-Grad.Rate))
y_train = df_college_train$Grad.Rate
x_test = data.matrix(select(df_college_test, -Grad.Rate))
lambda_finder = cv.glmnet(x_train, y_train, alpha=1)
best_lambda = lambda_finder$lambda.min

lasso_model = glmnet(x_train, y_train, alpha=0, lambda=best_lambda)
```

```{r}
df_college_test$Grad.Rate.Predicted = predict(lasso_model, s=best_lambda, newx=x_test)
df_college_test = mutate(df_college_test,
                         'Error'=Grad.Rate.Predicted-Grad.Rate,
                         'Error_Squared'=Error*Error)
print(paste('The test MSE with the lasso model is: ', mean(df_college_test$Error_Squared)))
coef(lasso_model)
```

(d) Among those that are not predicted well, do you notice any common trend 
shared between the colleges?  

#############################################################################################
The colleges tend to be low graduation rate colleges. They also tend to be skewed a bit more towards the smaller side but that's a bit less significant.
#############################################################################################
